<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Neural Networks vs Classical ML Training</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #1e3c72 0%, #2a5298 100%);
            min-height: 100vh;
            display: flex;
            align-items: center;
            justify-content: center;
        }

        .presentation-container {
            width: 95%;
            max-width: 1200px;
            background: white;
            border-radius: 20px;
            box-shadow: 0 20px 60px rgba(0, 0, 0, 0.3);
            overflow: hidden;
        }

        .slide {
            display: none;
            padding: 50px;
            min-height: 700px;
            position: relative;
        }

        .slide.active {
            display: block;
            animation: slideIn 0.5s ease-in-out;
        }

        @keyframes slideIn {
            from {
                opacity: 0;
                transform: translateX(20px);
            }
            to {
                opacity: 1;
                transform: translateX(0);
            }
        }

        .slide h1 {
            color: #2d3748;
            font-size: 2.5em;
            margin-bottom: 30px;
            text-align: center;
            border-bottom: 4px solid #2a5298;
            padding-bottom: 20px;
        }

        .slide h2 {
            color: #4a5568;
            font-size: 2em;
            margin: 30px 0 20px 0;
            display: flex;
            align-items: center;
        }

        .slide h3 {
            color: #718096;
            font-size: 1.4em;
            margin: 20px 0 15px 0;
        }

        .comparison-container {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 30px;
            margin: 20px 0;
        }

        .classical-ml {
            background: linear-gradient(135deg, #f0f9ff 0%, #dbeafe 100%);
            padding: 25px;
            border-radius: 15px;
            border-left: 6px solid #3b82f6;
        }

        .neural-net {
            background: linear-gradient(135deg, #f0fdf4 0%, #dcfce7 100%);
            padding: 25px;
            border-radius: 15px;
            border-left: 6px solid #10b981;
        }

        .code-block {
            background: #1a202c;
            color: #e2e8f0;
            padding: 25px;
            border-radius: 12px;
            font-family: 'Consolas', 'Monaco', 'Courier New', monospace;
            margin: 20px 0;
            overflow-x: auto;
            font-size: 13px;
            line-height: 1.6;
            border: 1px solid #2d3748;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
        }

        .code-block code {
            color: #68d391;
        }

        .code-block .comment {
            color: #a0aec0;
            font-style: italic;
        }

        .code-block .keyword {
            color: #f6ad55;
        }

        .code-block .string {
            color: #9f7aea;
        }

        .navigation {
            position: fixed;
            bottom: 30px;
            left: 50%;
            transform: translateX(-50%);
            display: flex;
            gap: 15px;
            z-index: 1000;
        }

        .nav-btn {
            padding: 12px 24px;
            background: #2a5298;
            color: white;
            border: none;
            border-radius: 25px;
            cursor: pointer;
            font-size: 16px;
            transition: all 0.3s ease;
            box-shadow: 0 4px 15px rgba(42, 82, 152, 0.4);
        }

        .nav-btn:hover {
            background: #1e3c72;
            transform: translateY(-2px);
            box-shadow: 0 6px 20px rgba(42, 82, 152, 0.6);
        }

        .nav-btn:disabled {
            background: #a0aec0;
            cursor: not-allowed;
            transform: none;
            box-shadow: none;
        }

        .slide-counter {
            position: absolute;
            top: 20px;
            right: 20px;
            background: #2a5298;
            color: white;
            padding: 8px 16px;
            border-radius: 20px;
            font-size: 14px;
        }

        .title-slide {
            text-align: center;
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: center;
        }

        .title-slide h1 {
            font-size: 3.5em;
            background: linear-gradient(135deg, #1e3c72 0%, #2a5298 100%);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            border: none;
            margin-bottom: 20px;
        }

        .title-slide p {
            font-size: 1.3em;
            color: #718096;
            margin: 10px 0;
        }

        .highlight {
            background: linear-gradient(135deg, #fef5e7 0%, #fed7aa 100%);
            padding: 20px;
            border-radius: 10px;
            margin: 20px 0;
            border-left: 4px solid #ed8936;
        }

        .comparison-table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            font-size: 14px;
        }

        .comparison-table th,
        .comparison-table td {
            border: 1px solid #e2e8f0;
            padding: 12px;
            text-align: left;
            vertical-align: top;
        }

        .comparison-table th {
            background: #f7fafc;
            font-weight: bold;
        }

        .pros-cons {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin: 20px 0;
        }

        .pros {
            background: linear-gradient(135deg, #f0fdf4 0%, #dcfce7 100%);
            padding: 20px;
            border-radius: 10px;
            border-left: 4px solid #10b981;
        }

        .cons {
            background: linear-gradient(135deg, #fef2f2 0%, #fecaca 100%);
            padding: 20px;
            border-radius: 10px;
            border-left: 4px solid #ef4444;
        }

        ul {
            margin: 10px 0 10px 20px;
        }

        li {
            margin: 8px 0;
            line-height: 1.6;
        }

        .visual-diagram {
            background: linear-gradient(135deg, #f7fafc 0%, #edf2f7 100%);
            border: 2px solid #e2e8f0;
            border-radius: 12px;
            padding: 25px;
            margin: 20px 0;
            text-align: center;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.05);
        }

        .flow-step {
            display: inline-block;
            background: #3b82f6;
            color: white;
            padding: 10px 20px;
            border-radius: 20px;
            margin: 5px;
            font-weight: bold;
        }

        .arrow {
            font-size: 2em;
            color: #6b7280;
            margin: 0 10px;
        }

        .key-concept {
            background: linear-gradient(135deg, #e0e7ff 0%, #c7d2fe 100%);
            padding: 20px;
            border-radius: 10px;
            margin: 15px 0;
            border-left: 4px solid #6366f1;
        }

        .metric-box {
            background: white;
            border: 2px solid #e5e7eb;
            border-radius: 8px;
            padding: 15px;
            text-align: center;
            margin: 10px;
        }

        .metric-value {
            font-size: 2em;
            font-weight: bold;
            color: #2a5298;
        }

        .metric-label {
            font-size: 0.9em;
            color: #6b7280;
            margin-top: 5px;
        }

        .quiz-container {
            background: linear-gradient(135deg, #f8fafc 0%, #edf2f7 100%);
            border: 2px solid #e2e8f0;
            border-radius: 12px;
            padding: 25px;
            margin: 20px 0;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.05);
        }

        .quiz-question {
            background: white;
            border: 1px solid #e2e8f0;
            border-radius: 10px;
            padding: 20px;
            margin: 15px 0;
            box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
        }

        .quiz-answer {
            background: linear-gradient(135deg, #f0fdf4 0%, #dcfce7 100%);
            border: 1px solid #10b981;
            border-radius: 10px;
            padding: 20px;
            margin: 15px 0;
            display: none;
            box-shadow: 0 1px 3px rgba(16, 185, 129, 0.1);
        }

        .show-answer {
            background: #3b82f6;
            color: white;
            border: none;
            padding: 8px 16px;
            border-radius: 5px;
            cursor: pointer;
            margin-top: 10px;
        }
    </style>
</head>
<body>
    <div class="presentation-container">
        <div class="slide-counter">
            <span id="currentSlide">1</span> / <span id="totalSlides">12</span>
        </div>

        <!-- Slide 1: Title -->
        <div class="slide active title-slide">
            <h1>Neural Networks vs Classical ML</h1>
            <p>Training Paradigms, Epochs, and Batch Processing</p>
            <p style="margin-top: 40px; font-size: 1.1em;">ü§ñ Understanding Different Learning Approaches</p>
            <div style="margin-top: 30px; font-size: 0.9em; color: #718096;">
                <p>üéØ Learning Objectives:</p>
                <ul style="text-align: left; display: inline-block;">
                    <li>Understand training differences between classical ML and neural networks</li>
                    <li>Learn about epochs, batch processing, and iterative learning</li>
                    <li>Compare resource requirements and use cases</li>
                    <li>Master practical implementation strategies</li>
                </ul>
            </div>
        </div>

        <!-- Slide 2: Overview -->
        <div class="slide">
            <h1>Training Paradigms Overview</h1>
            
            <div class="comparison-container">
                <div class="classical-ml">
                    <h3>üîµ Classical Machine Learning</h3>
                    <ul>
                        <li><strong>One-shot learning:</strong> Train once on entire dataset</li>
                        <li><strong>Batch processing:</strong> All data processed simultaneously</li>
                        <li><strong>Direct optimization:</strong> Closed-form or iterative solutions</li>
                        <li><strong>Feature engineering:</strong> Manual feature extraction</li>
                        <li><strong>Deterministic:</strong> Same result every time</li>
                    </ul>
                </div>

                <div class="neural-net">
                    <h3>üü¢ Neural Networks</h3>
                    <ul>
                        <li><strong>Iterative learning:</strong> Multiple passes (epochs) through data</li>
                        <li><strong>Mini-batch processing:</strong> Small chunks of data</li>
                        <li><strong>Gradient descent:</strong> Gradual parameter updates</li>
                        <li><strong>Automatic features:</strong> Learned representations</li>
                        <li><strong>Stochastic:</strong> Results may vary between runs</li>
                    </ul>
                </div>
            </div>

            <div class="highlight">
                <strong>Key Insight:</strong> Classical ML typically trains once and is done, while neural networks require multiple iterations through the data to gradually learn complex patterns.
            </div>

            <div class="visual-diagram">
                <h4>Training Flow Comparison</h4>
                <div style="margin: 20px 0;">
                    <strong>Classical ML:</strong> Data ‚Üí Feature Engineering ‚Üí Algorithm ‚Üí Model ‚úÖ
                </div>
                <div>
                    <strong>Neural Networks:</strong> Data ‚Üí Batch 1 ‚Üí Update ‚Üí Batch 2 ‚Üí Update ‚Üí ... ‚Üí Epoch 1 ‚Üí Epoch 2 ‚Üí ... ‚Üí Model ‚úÖ
                </div>
            </div>
        </div>

        <!-- Slide 3: Classical ML Training -->
        <div class="slide">
            <h1>Classical ML Training Process</h1>
            
            <div class="key-concept">
                <h3>üéØ One-Shot Learning</h3>
                <p>Classical algorithms typically process the entire dataset at once to find the optimal solution.</p>
            </div>

            <div class="code-block">
# Classical ML Training Examples

# 1. Linear Regression - Closed Form Solution
from sklearn.linear_model import LinearRegression
import numpy as np

X_train = np.random.randn(1000, 5)  # All training data
y_train = np.random.randn(1000)

model = LinearRegression()
model.fit(X_train, y_train)  # Single training step - DONE!

# 2. SVM - Iterative but still batch processing
from sklearn.svm import SVC

svm = SVC()
svm.fit(X_train, y_train)  # Processes entire dataset

# 3. Random Forest - Ensemble of decision trees
from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(n_estimators=100)
rf.fit(X_train, y_train)  # Builds all trees using full dataset
            </div>

            <div class="pros-cons">
                <div class="pros">
                    <h4>‚úÖ Advantages</h4>
                    <ul>
                        <li>Simple training process</li>
                        <li>Fast training (usually)</li>
                        <li>Deterministic results</li>
                        <li>No hyperparameter tuning for epochs</li>
                        <li>Memory efficient for small datasets</li>
                    </ul>
                </div>
                <div class="cons">
                    <h4>‚ùå Limitations</h4>
                    <ul>
                        <li>Limited to engineered features</li>
                        <li>Cannot handle very large datasets</li>
                        <li>Less flexible for complex patterns</li>
                        <li>Requires all data in memory</li>
                        <li>Limited scalability</li>
                    </ul>
                </div>
            </div>
        </div>

        <!-- Slide 4: Neural Network Training -->
        <div class="slide">
            <h1>Neural Network Training Process</h1>
            
            <div class="key-concept">
                <h3>üîÑ Iterative Learning with Epochs</h3>
                <p>Neural networks learn through multiple complete passes (epochs) through the training data, updating weights incrementally.</p>
            </div>

            <div class="code-block">
# Neural Network Training with Epochs
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

# Create dataset and dataloader
dataset = TensorDataset(X_tensor, y_tensor)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

model = nn.Sequential(
    nn.Linear(5, 64),
    nn.ReLU(),
    nn.Linear(64, 1)
)

optimizer = optim.Adam(model.parameters(), lr=0.001)
criterion = nn.MSELoss()

# Training loop with epochs
num_epochs = 100
for epoch in range(num_epochs):  # Multiple passes through data
    epoch_loss = 0
    
    for batch_X, batch_y in dataloader:  # Process in small batches
        # Forward pass
        predictions = model(batch_X)
        loss = criterion(predictions, batch_y)
        
        # Backward pass and update
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        epoch_loss += loss.item()
    
    if epoch % 10 == 0:
        print(f'Epoch {epoch}, Loss: {epoch_loss/len(dataloader):.4f}')
            </div>
        </div>

        <!-- Slide 5: Understanding Epochs -->
        <div class="slide">
            <h1>What are Epochs?</h1>
            
            <div class="key-concept">
                <h3>üìö Epoch Definition</h3>
                <p>An <strong>epoch</strong> is one complete pass through the entire training dataset. Neural networks typically require many epochs to converge.</p>
            </div>

            <div style="display: grid; grid-template-columns: 1fr 2fr; gap: 30px;">
                <div>
                    <div class="metric-box">
                        <div class="metric-value">1</div>
                        <div class="metric-label">Epoch</div>
                    </div>
                    <div class="metric-box">
                        <div class="metric-value">1000</div>
                        <div class="metric-label">Samples</div>
                    </div>
                    <div class="metric-box">
                        <div class="metric-value">32</div>
                        <div class="metric-label">Batch Size</div>
                    </div>
                    <div class="metric-box">
                        <div class="metric-value">32</div>
                        <div class="metric-label">Batches per Epoch</div>
                    </div>
                </div>

                <div>
                    <h4>Epoch Progression:</h4>
                    <div class="code-block">
# Epoch 1: Model sees all 1000 samples
# Batch 1: samples 0-31    ‚Üí update weights
# Batch 2: samples 32-63   ‚Üí update weights  
# ...
# Batch 32: samples 992-999 ‚Üí update weights
# END OF EPOCH 1

# Epoch 2: Model sees all 1000 samples AGAIN
# (shuffled order)
# Batch 1: samples 234-265  ‚Üí update weights
# Batch 2: samples 67-98    ‚Üí update weights
# ...
# END OF EPOCH 2

# Continue for 50-100+ epochs until convergence
                    </div>
                </div>
            </div>

            <div class="highlight">
                <strong>Why Multiple Epochs?</strong> Neural networks learn gradually. Each epoch allows the model to refine its understanding of the data patterns. Early epochs learn basic patterns, later epochs fine-tune complex relationships.
            </div>
        </div>

        <!-- Slide 6: Batch Size Impact -->
        <div class="slide">
            <h1>Batch Size in Neural Networks</h1>
            
            <div class="key-concept">
                <h3>üì¶ Batch Size Definition</h3>
                <p>Number of samples processed together before updating model weights. Critical hyperparameter affecting training dynamics.</p>
            </div>

            <table class="comparison-table">
                <tr>
                    <th>Batch Size</th>
                    <th>Description</th>
                    <th>Memory Usage</th>
                    <th>Training Speed</th>
                    <th>Gradient Quality</th>
                    <th>Generalization</th>
                </tr>
                <tr>
                    <td><strong>Small (1-32)</strong></td>
                    <td>Stochastic/Mini-batch</td>
                    <td>Low</td>
                    <td>Fast per update</td>
                    <td>Noisy gradients</td>
                    <td>Better</td>
                </tr>
                <tr>
                    <td><strong>Medium (32-512)</strong></td>
                    <td>Mini-batch (common)</td>
                    <td>Moderate</td>
                    <td>Balanced</td>
                    <td>Stable gradients</td>
                    <td>Good</td>
                </tr>
                <tr>
                    <td><strong>Large (512+)</strong></td>
                    <td>Large batch</td>
                    <td>High</td>
                    <td>Slow per update</td>
                    <td>Smooth gradients</td>
                    <td>May overfit</td>
                </tr>
                <tr>
                    <td><strong>Full Dataset</strong></td>
                    <td>Batch gradient descent</td>
                    <td>Very High</td>
                    <td>Very slow</td>
                    <td>Perfect gradients</td>
                    <td>Often poor</td>
                </tr>
            </table>

            <div class="code-block">
# Batch Size Examples
import torch
from torch.utils.data import DataLoader

dataset = TensorDataset(X, y)

# Small batch - more updates per epoch, noisier gradients
small_loader = DataLoader(dataset, batch_size=16, shuffle=True)

# Medium batch - balanced approach (most common)
medium_loader = DataLoader(dataset, batch_size=64, shuffle=True)

# Large batch - fewer updates, smoother gradients
large_loader = DataLoader(dataset, batch_size=256, shuffle=True)

# Impact on training:
# Small batch: 1000/16 = 62.5 updates per epoch
# Medium batch: 1000/64 = 15.6 updates per epoch  
# Large batch: 1000/256 = 3.9 updates per epoch
            </div>
        </div>

        <!-- Slide 7: CNN Specific Considerations -->
        <div class="slide">
            <h1>CNN Training Specifics</h1>
            
            <div class="key-concept">
                <h3>üñºÔ∏è Convolutional Neural Networks</h3>
                <p>CNNs have additional considerations due to spatial data and memory requirements for feature maps.</p>
            </div>

            <div class="comparison-container">
                <div class="classical-ml">
                    <h4>üîµ Classical Computer Vision</h4>
                    <ul>
                        <li>Hand-crafted features (HOG, SIFT)</li>
                        <li>Fixed feature extraction</li>
                        <li>Train classifier on features</li>
                        <li>Process images one at a time</li>
                    </ul>
                </div>

                <div class="neural-net">
                    <h4>üü¢ CNN Training</h4>
                    <ul>
                        <li>Learned hierarchical features</li>
                        <li>Spatial weight sharing</li>
                        <li>Backpropagation through conv layers</li>
                        <li>Batch processing with 4D tensors</li>
                    </ul>
                </div>
            </div>

            <div class="code-block">
# CNN Training Example
import torch.nn as nn

class SimpleCNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv_layers = nn.Sequential(
            nn.Conv2d(3, 32, 3, padding=1),    # Learn 32 filters
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(32, 64, 3, padding=1),   # Learn 64 filters  
            nn.ReLU(),
            nn.MaxPool2d(2),
        )
        self.classifier = nn.Sequential(
            nn.Linear(64 * 8 * 8, 128),
            nn.ReLU(), 
            nn.Linear(128, 10)
        )
    
    def forward(self, x):
        # x shape: (batch_size, 3, 32, 32) - CIFAR-10 images
        x = self.conv_layers(x)  # Learn spatial features
        x = x.view(x.size(0), -1)  # Flatten for classifier
        return self.classifier(x)

# Training with image batches
dataloader = DataLoader(dataset, batch_size=64, shuffle=True)
# Each batch: torch.Size([64, 3, 32, 32]) - 64 RGB images

for epoch in range(50):  # CNNs often need more epochs
    for batch_images, batch_labels in dataloader:
        # Process 64 images simultaneously
        outputs = cnn_model(batch_images)
        loss = criterion(outputs, batch_labels)
        # ... backprop and update
            </div>

            <div class="highlight">
                <strong>CNN Memory Consideration:</strong> Batch size limited by GPU memory. Feature maps consume significant memory, especially in early layers. Common to use smaller batches (16-64) for high-resolution images.
            </div>
        </div>

        <!-- Slide 8: Training Time Comparison -->
        <div class="slide">
            <h1>Training Time & Resource Comparison</h1>
            
            <div style="display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 20px;">
                <div class="classical-ml">
                    <h4>üîµ Classical ML</h4>
                    <div class="metric-box">
                        <div class="metric-value">Minutes</div>
                        <div class="metric-label">Training Time</div>
                    </div>
                    <div class="metric-box">
                        <div class="metric-value">CPU</div>
                        <div class="metric-label">Hardware</div>
                    </div>
                    <div class="metric-box">
                        <div class="metric-value">GB</div>
                        <div class="metric-label">Memory</div>
                    </div>
                </div>

                <div class="neural-net">
                    <h4>üü¢ Standard NN</h4>
                    <div class="metric-box">
                        <div class="metric-value">Hours</div>
                        <div class="metric-label">Training Time</div>
                    </div>
                    <div class="metric-box">
                        <div class="metric-value">GPU</div>
                        <div class="metric-label">Hardware</div>
                    </div>
                    <div class="metric-box">
                        <div class="metric-value">10+ GB</div>
                        <div class="metric-label">Memory</div>
                    </div>
                </div>

                <div style="background: linear-gradient(135deg, #fdf2f8 0%, #fce7f3 100%); padding: 25px; border-radius: 15px; border-left: 6px solid #ec4899;">
                    <h4>üîÆ Deep CNN</h4>
                    <div class="metric-box">
                        <div class="metric-value">Days</div>
                        <div class="metric-label">Training Time</div>
                    </div>
                    <div class="metric-box">
                        <div class="metric-value">Multi-GPU</div>
                        <div class="metric-label">Hardware</div>
                    </div>
                    <div class="metric-box">
                        <div class="metric-value">100+ GB</div>
                        <div class="metric-label">Memory</div>
                    </div>
                </div>
            </div>

            <div class="code-block">
# Training Time Examples (rough estimates)

# Classical ML - Scikit-learn Random Forest
# Dataset: 100K samples, 20 features
start_time = time.time()
rf = RandomForestClassifier(n_estimators=100)
rf.fit(X_train, y_train)  # ~30 seconds to 2 minutes
print(f"Training time: {time.time() - start_time:.2f} seconds")

# Neural Network - PyTorch MLP
# Dataset: 100K samples, 20 features ‚Üí 128 ‚Üí 64 ‚Üí 10 classes
for epoch in range(100):  # ~10-30 minutes total
    for batch in dataloader:
        # ... training code
        
# CNN - ResNet50 on ImageNet
# Dataset: 1.2M images, 1000 classes
for epoch in range(90):   # ~1-2 weeks on single GPU
    for batch in dataloader:
        # ... training code
            </div>

            <div class="highlight">
                <strong>Scaling Reality:</strong> Classical ML scales linearly with data size. Neural networks scale with data size √ó epochs √ó model complexity. This is why neural networks require specialized hardware and parallel processing.
            </div>
        </div>

        <!-- Slide 9: When to Use What -->
        <div class="slide">
            <h1>When to Use Each Approach</h1>
            
            <table class="comparison-table">
                <tr>
                    <th>Scenario</th>
                    <th>Classical ML</th>
                    <th>Neural Networks</th>
                    <th>CNNs</th>
                </tr>
                <tr>
                    <td><strong>Small Dataset</strong></td>
                    <td>‚úÖ Preferred</td>
                    <td>‚ö†Ô∏è Risk of overfitting</td>
                    <td>‚ùå Likely to overfit</td>
                </tr>
                <tr>
                    <td><strong>Tabular Data</strong></td>
                    <td>‚úÖ Excellent choice</td>
                    <td>‚ö†Ô∏è Can work well</td>
                    <td>‚ùå Not suitable</td>
                </tr>
                <tr>
                    <td><strong>Image Classification</strong></td>
                    <td>‚ö†Ô∏è Limited accuracy</td>
                    <td>‚úÖ Good for simple tasks</td>
                    <td>‚úÖ State-of-the-art</td>
                </tr>
                <tr>
                    <td><strong>Large Dataset</strong></td>
                    <td>‚ö†Ô∏è May be slow</td>
                    <td>‚úÖ Excellent</td>
                    <td>‚úÖ Excellent</td>
                </tr>
                <tr>
                    <td><strong>Limited Computing Resources</strong></td>
                    <td>‚úÖ Very efficient</td>
                    <td>‚ö†Ô∏è Moderate resources</td>
                    <td>‚ùå High resource needs</td>
                </tr>
                <tr>
                    <td><strong>Need Interpretability</strong></td>
                    <td>‚úÖ Highly interpretable</td>
                    <td>‚ö†Ô∏è Limited interpretability</td>
                    <td>‚ùå Black box</td>
                </tr>
                <tr>
                    <td><strong>Quick Prototyping</strong></td>
                    <td>‚úÖ Very fast</td>
                    <td>‚ö†Ô∏è Moderate time</td>
                    <td>‚ùå Time consuming</td>
                </tr>
            </table>

            <div class="highlight">
                <strong>Rule of Thumb:</strong>
                <ul>
                    <li><strong>Classical ML:</strong> Start here for structured/tabular data, small datasets, or when you need interpretability</li>
                    <li><strong>Neural Networks:</strong> Use for complex patterns, large datasets, or when classical ML plateaus</li>
                    <li><strong>CNNs:</strong> Essential for computer vision tasks, especially with large image datasets</li>
                </ul>
            </div>
        </div>

        <!-- Slide 10: Summary -->
        <div class="slide">
            <h1>Key Takeaways</h1>
            
            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 30px;">
                <div>
                    <h3>üéØ Training Paradigms</h3>
                    <div class="key-concept">
                        <h4>Classical ML</h4>
                        <ul>
                            <li>One-shot learning</li>
                            <li>Full batch processing</li>
                            <li>Deterministic training</li>
                            <li>Feature engineering required</li>
                        </ul>
                    </div>
                    <div class="key-concept">
                        <h4>Neural Networks</h4>
                        <ul>
                            <li>Iterative learning with epochs</li>
                            <li>Mini-batch processing</li>
                            <li>Gradient-based optimization</li>
                            <li>Automatic feature learning</li>
                        </ul>
                    </div>
                </div>

                <div>
                    <h3>‚ö° Performance & Resources</h3>
                    <div class="key-concept">
                        <h4>Training Time</h4>
                        <ul>
                            <li>Classical ML: Minutes to hours</li>
                            <li>Neural Networks: Hours to days</li>
                            <li>CNNs: Days to weeks</li>
                        </ul>
                    </div>
                    <div class="key-concept">
                        <h4>Resource Requirements</h4>
                        <ul>
                            <li>Classical ML: CPU, low memory</li>
                            <li>Neural Networks: GPU recommended</li>
                            <li>CNNs: High-end GPU required</li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="highlight">
                <h3>üéì Learning Path Recommendation</h3>
                <ol>
                    <li><strong>Start with Classical ML:</strong> Understand basic concepts, feature engineering, and model evaluation</li>
                    <li><strong>Progress to Neural Networks:</strong> Learn about epochs, batch processing, and gradient descent</li>
                    <li><strong>Specialize in CNNs:</strong> Master computer vision and spatial data processing</li>
                    <li><strong>Advanced Topics:</strong> Explore transformers, reinforcement learning, and generative models</li>
                </ol>
            </div>
        </div>

        <!-- Slide 11: Interactive Quiz -->
        <div class="slide">
            <h1>Quick Knowledge Check</h1>
            
            <div class="key-concept">
                <h3>ü§î Test Your Understanding</h3>
                <p>Answer these questions to reinforce your learning:</p>
            </div>

            <div class="quiz-container">
                <div class="quiz-question">
                    <h4>Question 1: Training Process</h4>
                    <p><strong>Which approach processes the entire dataset at once?</strong></p>
                    <ul>
                        <li>A) Neural Networks</li>
                        <li>B) Classical ML</li>
                        <li>C) Both approaches</li>
                        <li>D) Neither approach</li>
                    </ul>
                    <button class="show-answer" onclick="toggleAnswer('answer1')">Show Answer</button>
                    <div id="answer1" class="quiz-answer">
                        <strong>Answer: B) Classical ML</strong><br>
                        Classical ML typically processes all data in one batch, while neural networks use mini-batches over multiple epochs.
                    </div>
                </div>

                <div class="quiz-question">
                    <h4>Question 2: Epochs</h4>
                    <p><strong>What is an epoch in neural network training?</strong></p>
                    <ul>
                        <li>A) One weight update</li>
                        <li>B) One complete pass through the dataset</li>
                        <li>C) One batch processed</li>
                        <li>D) One layer of the network</li>
                    </ul>
                    <button class="show-answer" onclick="toggleAnswer('answer2')">Show Answer</button>
                    <div id="answer2" class="quiz-answer">
                        <strong>Answer: B) One complete pass through the dataset</strong><br>
                        An epoch means the model has seen every training sample exactly once.
                    </div>
                </div>
            </div>
        </div>

        <!-- Slide 12: Practical Tips -->
        <div class="slide">
            <h1>Practical Implementation Tips</h1>
            
            <div class="comparison-container">
                <div class="classical-ml">
                    <h3>üîµ Classical ML Best Practices</h3>
                    <div class="key-concept">
                        <h4>Data Preparation</h4>
                        <ul>
                            <li>Handle missing values appropriately</li>
                            <li>Scale/normalize numerical features</li>
                            <li>Encode categorical variables</li>
                            <li>Remove outliers if necessary</li>
                        </ul>
                    </div>
                    <div class="key-concept">
                        <h4>Model Selection</h4>
                        <ul>
                            <li>Start with simple models (linear/logistic regression)</li>
                            <li>Use cross-validation for evaluation</li>
                            <li>Try ensemble methods for better performance</li>
                            <li>Consider interpretability requirements</li>
                        </ul>
                    </div>
                </div>

                <div class="neural-net">
                    <h3>üü¢ Neural Network Best Practices</h3>
                    <div class="key-concept">
                        <h4>Architecture Design</h4>
                        <ul>
                            <li>Start with simple architectures</li>
                            <li>Use appropriate activation functions</li>
                            <li>Include dropout for regularization</li>
                            <li>Monitor training/validation curves</li>
                        </ul>
                    </div>
                    <div class="key-concept">
                        <h4>Training Strategy</h4>
                        <ul>
                            <li>Choose appropriate learning rate</li>
                            <li>Use learning rate scheduling</li>
                            <li>Implement early stopping</li>
                            <li>Save best model checkpoints</li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="highlight">
                <h3>üéØ Key Success Factors</h3>
                <ul>
                    <li><strong>Data Quality:</strong> Garbage in, garbage out - clean your data thoroughly</li>
                    <li><strong>Feature Engineering:</strong> Critical for classical ML, less important for deep learning</li>
                    <li><strong>Hyperparameter Tuning:</strong> Use grid search, random search, or Bayesian optimization</li>
                    <li><strong>Evaluation:</strong> Use appropriate metrics and validation strategies</li>
                    <li><strong>Monitoring:</strong> Track training progress and watch for overfitting</li>
                </ul>
            </div>
        </div>
    </div>

    <div class="navigation">
        <button class="nav-btn" onclick="previousSlide()">‚Üê Previous</button>
        <button class="nav-btn" onclick="nextSlide()">Next ‚Üí</button>
    </div>

    <script>
        let currentSlideIndex = 0;
        const slides = document.querySelectorAll('.slide');
        const totalSlides = slides.length;

        function showSlide(index) {
            slides.forEach(slide => slide.classList.remove('active'));
            slides[index].classList.add('active');
            document.getElementById('currentSlide').textContent = index + 1;
            document.getElementById('totalSlides').textContent = totalSlides;
            
            const prevBtn = document.querySelector('.nav-btn:first-child');
            const nextBtn = document.querySelector('.nav-btn:last-child');
            prevBtn.disabled = index === 0;
            nextBtn.disabled = index === totalSlides - 1;
        }

        function nextSlide() {
            if (currentSlideIndex < totalSlides - 1) {
                currentSlideIndex++;
                showSlide(currentSlideIndex);
            }
        }

        function previousSlide() {
            if (currentSlideIndex > 0) {
                currentSlideIndex--;
                showSlide(currentSlideIndex);
            }
        }

        function toggleAnswer(answerId) {
            const answer = document.getElementById(answerId);
            answer.style.display = answer.style.display === 'block' ? 'none' : 'block';
        }

        document.addEventListener('keydown', function(event) {
            if (event.key === 'ArrowRight' || event.key === ' ') {
                nextSlide();
            } else if (event.key === 'ArrowLeft') {
                previousSlide();
            }
        });

        showSlide(0);
    </script>
</body>
</html>